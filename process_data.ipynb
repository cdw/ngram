{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from utils import get_tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of documents\n",
    "* data/trainW_raw.txt: word list from https://github.com/dwyl/english-words/blob/master/words_alpha.txt (370103 words)\n",
    "* data/trainT_raw.txt: text from George Martin's [A Storm of Swords](https://en.wikipedia.org/wiki/A_Storm_of_Swords) (~424k words*)\n",
    "* data/test1_raw.txt: text from George Martin's [A Dance with Dragons](https://fr.wikipedia.org/wiki/A_Dance_with_Dragons) (~422k words*)\n",
    "* data/test2_raw.txt: text from Margaret Mitchell's [Gone with the Wind](https://en.wikipedia.org/wiki/Gone_with_the_Wind_(novel)) (~418k words*)\n",
    "\n",
    "\\* https://blog.nathanbransford.com/2018/04/all-about-novel-word-counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize each document and save it to a separate text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/trainW_raw.txt') as read_handle,\\\n",
    "     open('data/trainW_token.txt', 'w') as write_handle,\\\n",
    "     open('data/trainW_token_end.txt', 'w') as write_handle_end:\n",
    "    words = read_handle.read().splitlines()\n",
    "    word_list_count = len(words)\n",
    "    write_handle.write(','.join(words))\n",
    "    write_handle_end.write(','.join(words)+',<END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainT, test1, test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These texts will be trimmed at the end so that the number of tokens of each text match that of trainW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenized_sentences(content, split_newline):\n",
    "    # Only consider sequence of alphanumeric characters, dash (gray-brown), and apostrophe (that's) as token\n",
    "    tokenizer = RegexpTokenizer(r'[-\\'\\w]+')\n",
    "    \n",
    "    # Split document into paragraphs (separated by new lines)\n",
    "    # If raw text goes to new line arbitrarily, disable splitting paragraphs by new line (split_newline=False)\n",
    "    if split_newline:\n",
    "        paragraphs = content.split('\\n')\n",
    "    else:\n",
    "        paragraphs = [content]\n",
    "    # Split each paragraph into sentences (separated by punctuation characters as defined by sent_tokenize),\n",
    "    # then tokens (using RegexpTokenizer with defined regexp pattern for token)\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = sent_tokenize(paragraph)\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "            if tokenized_sentence:\n",
    "                yield tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_characters(content):\n",
    "    # Replace characters that messes up tokenizers\n",
    "    replacement_rules = {'“': '\"', '”': '\"', '’': \"'\", '--': ','}\n",
    "    for symbol, replacement_symbol in replacement_rules.items():\n",
    "        content = content.replace(symbol, replacement_symbol)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_raw_text(raw_text_path, token_text_path, word_limit, split_newline, include_end_symbol):\n",
    "    with open(raw_text_path) as read_handle, open(token_text_path, 'w') as write_handle:\n",
    "        # Read raw text\n",
    "        content = read_handle.read().lower()\n",
    "        \n",
    "        # Replace characters that messes up tokenizers\n",
    "        content = replace_characters(content)\n",
    "        \n",
    "        # Generate tokenized sentence one at a time and write them to file until word limit is met\n",
    "        tokenized_sentence_generator = generate_tokenized_sentences(content, split_newline)\n",
    "        \n",
    "        for tokenized_sentence in tokenized_sentence_generator:\n",
    "            if include_end_symbol:\n",
    "                tokenized_sentence.append('<END>')\n",
    "            sentence_length = len(tokenized_sentence)\n",
    "            if sentence_length <= word_limit:\n",
    "                write_handle.write(','.join(tokenized_sentence))\n",
    "                write_handle.write('\\n')\n",
    "                word_limit -= len(tokenized_sentence)\n",
    "            else:\n",
    "                write_handle.write(','.join(tokenized_sentence[:word_limit]))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_raw_text('data/trainT_raw.txt', 'data/trainT_token.txt', word_list_count, split_newline=True, include_end_symbol=False)\n",
    "tokenize_raw_text('data/test1_raw.txt', 'data/test1_token.txt', word_list_count, split_newline=False, include_end_symbol=False)\n",
    "tokenize_raw_text('data/test2_raw.txt', 'data/test2_token.txt', word_list_count, split_newline=True, include_end_symbol=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_raw_text('data/trainT_raw.txt', 'data/trainT_token_end.txt', word_list_count+1, split_newline=True, include_end_symbol=True)\n",
    "tokenize_raw_text('data/test1_raw.txt', 'data/test1_token_end.txt', word_list_count+1, split_newline=False, include_end_symbol=True)\n",
    "tokenize_raw_text('data/test2_raw.txt', 'data/test2_token_end.txt', word_list_count+1, split_newline=True, include_end_symbol=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify token counts of tokenized texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token(tokenized_file_name):\n",
    "    return sum(len(tokenized_sentence) for tokenized_sentence in get_tokenized_sentences(tokenized_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/trainW_token.txt 370103\n",
      "data/trainW_token_end.txt 370104\n",
      "data/trainT_token.txt 370103\n",
      "data/trainT_token_end.txt 370104\n",
      "data/test1_token.txt 370103\n",
      "data/test1_token_end.txt 370104\n",
      "data/test2_token.txt 370103\n",
      "data/test2_token_end.txt 370104\n"
     ]
    }
   ],
   "source": [
    "file_stems = ['trainW', 'trainT', 'test1', 'test2']\n",
    "for file_stem in file_stems:\n",
    "    file_name = f'data/{file_stem}_token.txt'\n",
    "    file_name_end = f'data/{file_stem}_token_end.txt'\n",
    "    print(file_name, count_token(file_name))\n",
    "    print(file_name_end, count_token(file_name_end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}