{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of documents\n",
    "* data/trainW_raw.txt: word list from https://github.com/dwyl/english-words/blob/master/words_alpha.txt (370103 words)\n",
    "* data/trainT_raw.txt: text from George Martin's [A Storm of Swords](https://en.wikipedia.org/wiki/A_Storm_of_Swords) (~424k words*)\n",
    "* data/test1_raw.txt: text from George Martin's [A Dance with Dragons](https://fr.wikipedia.org/wiki/A_Dance_with_Dragons) (~422k words*)\n",
    "* data/test2_raw.txt: text from Margaret Mitchell's [Gone with the Wind](https://en.wikipedia.org/wiki/Gone_with_the_Wind_(novel)) (~418k words*)\n",
    "\n",
    "\\* https://blog.nathanbransford.com/2018/04/all-about-novel-word-counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize each document and save it to a separate text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/trainW_raw.txt') as read_handle, open('data/trainW_token.txt', 'w') as write_handle:\n",
    "    words = read_handle.read().splitlines()\n",
    "    word_list_count = len(words)\n",
    "    write_handle.write(','.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainT, test1, test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These texts will be trimmed at the end so that the number of tokens of each text match that of trainW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenized_sentences(content):\n",
    "    # Only consider sequence of alphanumeric characters, dash (gray-brown), and apostrophe (that's) as token\n",
    "    tokenizer = RegexpTokenizer(r'[-\\'\\w]+')\n",
    "    \n",
    "    # Split document into paragraphs (separated by new lines), \n",
    "    # then sentences (separated by punctuation characters as defined by sent_tokenize),\n",
    "    # then tokens (using RegexpTokenizer)\n",
    "    paragraphs = content.split('\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = sent_tokenize(paragraph)\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "            if tokenized_sentence:\n",
    "                yield tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_raw_text(raw_text_path, token_text_path, word_limit):\n",
    "    with open(raw_text_path) as read_handle, open(token_text_path, 'w') as write_handle:\n",
    "        # Read raw text\n",
    "        content = read_handle.read().lower()\n",
    "        \n",
    "        # Replace characters that messes up tokenizers\n",
    "        replacement_rules = {'“': '\"', '”': '\"', '’': \"'\", '--': ','}\n",
    "        for symbol, replacement_symbol in replacement_rules.items():\n",
    "            content = content.replace(symbol, replacement_symbol)\n",
    "        \n",
    "        # Generate tokenized sentence one at a time and write them to file until word limit is met\n",
    "        tokenized_sentence_generator = generate_tokenized_sentences(content)\n",
    "        \n",
    "        for tokenized_sentence in tokenized_sentence_generator:\n",
    "            sentence_length = len(tokenized_sentence)\n",
    "            if sentence_length <= word_limit:\n",
    "                write_handle.write(','.join(tokenized_sentence))\n",
    "                write_handle.write('\\n')\n",
    "                word_limit -= len(tokenized_sentence)\n",
    "            else:\n",
    "                write_handle.write(','.join(tokenized_sentence[:word_limit]))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_raw_text('data/trainT_raw.txt', 'data/trainT_token.txt', word_list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_raw_text('data/test1_raw.txt', 'data/test1_token.txt', word_list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_raw_text('data/test2_raw.txt', 'data/test2_token.txt', word_list_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify token counts of tokenized texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(tokenized_file_name):\n",
    "    with open(tokenized_file_name) as file_handle:\n",
    "        sentences = file_handle.read().splitlines()\n",
    "        for sentence in sentences:\n",
    "            if sentence:\n",
    "                tokenized_sentences = sentence.split(',')\n",
    "                yield tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token(tokenized_file_name):\n",
    "    return sum(len(tokenized_sentence) for tokenized_sentence in get_tokenized_sentences(tokenized_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_token('data/trainW_token.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370103"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_token('data/trainT_token.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_token('data/test1_token.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370103"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_token('data/test2_token.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
